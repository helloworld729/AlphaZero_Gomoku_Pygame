# v1
所以，模型到底是怎么训练的呢？
训练阶段：
「模型为 MCTS 提供先验知识（减少无效搜索），MCTS 为模型提供优化目标（提升模型性能）」，双向赋能，共同生成最优策略。

实战阶段：
「模型提供精准先验知识，MCTS 基于先验知识做深度推演，最终输出确定性最优落子」，不再追求探索性，只追求决策的准确性。


UCT 值用于选择推演路径，而最终落子是基于访问次数。这两者可能不一致！


② 理解难点

推演会对搜索树执行扩展操作，mcts全局共享，其扩展并不会因为棋盘的深拷贝解耦而解耦，即多次推演之间并非完全解耦。
例如同一落子时间步下，第N次推演执行了叶子节点A的扩展操作，第N+1看到的节点A就不是叶子节点了。
由于探索的存在，推演结束后 实际选择的节点可能是访问扩展过的，也可能仅扩展未访问。因此下一落子时间步推演的时候，
第一次推演可能面向的是叶子节点/非叶子节点。

虽然棋盘在深拷贝，但是树没有拷贝，一直在一颗上上推演。

注：mcts的策略函数，即policy_value_fn会对非法制进行过滤。



children: act -> TreeNode
state:    act -> player

策略评估的时候
AlphaZero (current_mcts_player) 是玩家1
Pure MCTS (pure_mcts_player) 是玩家2
win_ratio 计算的是 win_cnt[1]，也就是玩家1（AlphaZero）的胜率
对局安排：
通过 start_player=i % 2 交替先后手
当 i 为偶数时，玩家1（AlphaZero）先手
当 i 为奇数时，玩家2（Pure MCTS）先手




create a new repository on the command line
echo "# AlphaZero_Gomoku_Pygame" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/helloworld729/AlphaZero_Gomoku_Pygame.git
git push -u origin main

or push an existing repository from the command line
git remote add origin https://github.com/helloworld729/AlphaZero_Gomoku_Pygame.git
git branch -M main
git push -u origin main


# v2
先用2e-3训练1500个epoch，在用5e-5训练1500个epoch
