# AlphaZero 五子棋项目核心概念笔记

## 一、AlphaZero 训练与实战阶段的要点

### 训练阶段：
「模型为 MCTS 提供子节点的价值先验评估，MCTS 为模型提供优化目标（policy + value(0/1)）」

**核心机制**：
- **模型 → MCTS**：神经网络输出先验概率 P(s,a)，影响UCT，引导 MCTS 优先探索有价值的分支
- **MCTS → 模型**：通过大量模拟生成更准确的动作概率分布，作为神经网络的训练目标
- **协同效果**：模型和 MCTS 互相促进，共同生成最优策略(ppo架构)


### 关于落子
```
推演阶段：基于UCT口径在treeNode执行select，从而选择被访问的节点
落子阶段：直接选择访问次数最多的动作
```

---

## 二、MCTS 核心机制深度理解

### 1. UCT 与最终落子的关系

**重要概念**：
- **UCT 公式**：用于在 Selection 阶段选择推演路径
  ```
  UCT = Q(s,a) + c_puct × P(s,a) × sqrt(N(s)) / (1 + N(s,a))
  ```
- **最终落子**：基于根节点子节点的访问次数 N(s,a) 决定
  - 训练阶段：根据访问次数分布进行温度采样
  - 实战阶段：选择访问次数最多的动作

**为什么不直接用 Q 值？**
- 访问次数融合了"价值"和"不确定性"两个维度
- 访问次数多 = MCTS 认为这个分支值得反复探索
- 更鲁棒，不容易被单次高估的 Q 值误导

### 2. 搜索树共享机制（易混淆点）

**现象**：推演之间的搜索树是共享的，但棋盘状态是独立的

**详细说明**：
```
同一落子时间步下：
  - 棋盘状态：每次推演都会深拷贝，互不影响
  - 搜索树：全局共享，所有推演都在同一棵树上操作

结果：
  - 第 N 次推演扩展了叶子节点 A
  - 第 N+1 次推演看到的节点 A 已经不是叶子节点了
  - 搜索树逐步完善，后续推演可以利用之前的探索结果
```

**关键理解**：
- 虽然棋盘在深拷贝，但搜索树没有拷贝，一直在同一棵树上推演
- 这是 MCTS 的核心优势：**增量式探索，逐步逼近最优策略**
- 推演次数越多，搜索树越完善，决策越准确

**下一步推演的起点**：
- 可能是叶子节点（刚扩展但未访问）
- 也可能是非叶子节点（已扩展且已访问）
- 取决于 UCT 公式的探索-利用权衡

---

## 三、Negamax 框架：价值回溯的精妙设计

### 核心问题：为什么叶子节点的 leaf_value 要取负数？

**完整推理流程**：

#### 第一步：到达叶子节点
- 无论经过多少次路由，break 时的 node 肯定是叶子节点
- 叶子节点 = 没有子节点的节点

#### 第二步：视角切换的关键
- 能走到叶子节点 → 必定执行了 `board.do_move(action)`
- 此时 `board.current_player` 已经切换为叶子节点的**对手**

#### 第三步：策略评估
- 在叶子节点执行 `policy_value_fn(board)`
- 返回：
  - **策略分布 P(s,a)**：用于扩展子节点
  - **状态价值 V(s)**：从**当前玩家**（即叶子节点的对手）视角的价值

#### 第四步：价值转换（Negamax 核心）
- 神经网络返回的 V(s) 是从**对手视角**的价值
- 叶子节点需要记录**自己视角**的价值
- 因此：`leaf_value = -V(s)`

#### 第五步：递归回溯
```python
node.update_recursive(-leaf_value)
```
- 每次回溯都会取负号：`value = -value`
- 确保每个节点都从**自己的视角**记录价值
- 符号自动转换，代码简洁优雅

### Negamax 框架的优势

1. **统一视角**：所有节点都用相同的逻辑（从自己视角看价值）
2. **简化代码**：不需要区分玩家1和玩家2
3. **符号传播**：通过 `-value` 自动处理对手关系
4. **数学优雅**：利用零和博弈的对称性

### 数据结构说明
```python
# 搜索树节点
children: dict[action -> TreeNode]  # 动作到子节点的映射

# 棋盘状态
state: dict[action -> player]       # 动作（位置）到玩家（1或2）的映射
```

**注意**：MCTS 的策略函数 `policy_value_fn` 会自动过滤非法动作。

---

## 四、训练策略与超参数

### 分阶段训练建议
```
阶段一：高学习率快速探索
  - 学习率：2e-3
  - 轮数：1500 epoch
  - 目的：快速找到大致方向

阶段二：低学习率精细调优
  - 学习率：5e-5
  - 轮数：1500 epoch
  - 目的：稳定收敛，提升性能
```

### 当前项目配置
```python
learn_rate = 5e-5           # 直接使用低学习率
game_batch_num = 2000       # 总训练轮数
n_playout = 1000            # MCTS 模拟次数
pure_mcts_playout_num = 1000  # 评估对手难度
```

---

## 五、策略评估机制详解

### 评估角色分配
```
AlphaZero (current_mcts_player)  → 玩家1（被评估者）
Pure MCTS (pure_mcts_player)     → 玩家2（基准对手）

win_ratio = win_cnt[1] / n_games  # 计算玩家1（AlphaZero）的胜率
```

### 对局安排（公平性保证）
```python
通过 start_player = i % 2 交替先后手：
  - i 为偶数 → 玩家1（AlphaZero）先手
  - i 为奇数 → 玩家2（Pure MCTS）先手
```

**为什么要交替先后手？**
- 五子棋先手有一定优势
- 交替先后手确保评估公平
- 平均胜率更准确反映模型真实水平

### 搜索树重置的必要性

**关键问题**：为什么策略评估时需要重置搜索树？

**自我对弈阶段**（不需要重置）：
- 双方使用**同一个模型**
- 搜索树可以复用（因为先验知识相同）
- 提高训练效率

**策略评估阶段**（必须重置）：
- AlphaZero 使用**神经网络+MCTS**
- Pure MCTS 使用**纯 MCTS（无神经网络）**
- 两者的先验知识完全不同（"不是同一个师父教的"）
- 复用搜索树会混淆两种策略的信息
- **必须重置搜索树**，确保评估准确

---

## 六、Git 仓库管理命令

### 创建新仓库
```bash
echo "# AlphaZero_Gomoku_Pygame" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/helloworld729/AlphaZero_Gomoku_Pygame.git
git push -u origin main
```

### 推送现有仓库
```bash
git remote add origin https://github.com/helloworld729/AlphaZero_Gomoku_Pygame.git
git branch -M main
git push -u origin main
```

---

## 七、核心概念总结

### 1. AlphaZero = MCTS + 深度学习
- MCTS 提供强大的搜索能力
- 神经网络提供领域知识（先验概率 + 价值评估）
- 两者结合，突破单一方法的局限

### 2. 训练的本质
- **数据生成**：自我对弈产生训练样本
- **监督学习**：神经网络拟合 MCTS 的搜索结果
- **策略改进**：更强的网络指导更好的 MCTS
- **循环迭代**：棋力螺旋上升

### 3. 关键技术点
- **Negamax 框架**：优雅的价值传播机制
- **搜索树共享**：增量式探索，提高效率
- **温度参数**：平衡探索与利用
- **经验回放**：打破数据相关性
- **KL 散度约束**：防止训练崩溃
- **课程学习**：动态提升对手难度

### 4. 工程实践要点
- 定期保存模型（current + best）
- 定期评估性能（vs Pure MCTS）
- 监控训练指标（loss, entropy, KL, explained_var）
- 支持断点续训
- 可视化辅助理解（MCTS 树、训练曲线）

---

**最后更新时间**：2026-02-08

