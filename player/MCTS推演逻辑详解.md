# MCTS 推演逻辑详解

## 目录

1. [整体流程概览](#整体流程概览)
2. [四个核心步骤详解](#四个核心步骤详解)
3. [推演示例](#推演示例)
4. [关键代码解析](#关键代码解析)
5. [常见问题解答](#常见问题解答)

---

## 整体流程概览

### 高层流程图

```
┌─────────────────────────────────────────────────────────────┐
│                    MCTSPlayer.get_action()                  │
│                  (获取最优落子动作)                          │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│                 MCTS.get_move_probs(state)                  │
│              (执行 n_playout 次推演)                         │
│                                                             │
│  for i in range(n_playout):  # 例如 1000 次               │
│      state_copy = deepcopy(state)  # 深拷贝棋盘           │
│      _playout(state_copy)          # 单次推演              │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│                    MCTS._playout(state)                     │
│                     (单次推演)                               │
│                                                             │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  1️⃣ Selection (选择)                                │  │
│  │     从根节点向下选择，直到叶子节点                    │  │
│  │     node = root                                      │  │
│  │     while not node.is_leaf():                        │  │
│  │         action, node = node.select(c_puct)  # UCT   │  │
│  │         state.do_move(action)                        │  │
│  └──────────────────────────────────────────────────────┘  │
│                         │                                   │
│                         ▼                                   │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  2️⃣ Expansion (扩展)                                │  │
│  │     如果游戏未结束，扩展叶子节点                      │  │
│  │     probs, value = policy_value_fn(state)           │  │
│  │     node.expand(probs)                               │  │
│  └──────────────────────────────────────────────────────┘  │
│                         │                                   │
│                         ▼                                   │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  3️⃣ Evaluation (评估)                               │  │
│  │     神经网络评估当前局面价值                          │  │
│  │     value = V(s)  # 从对手视角                       │  │
│  │     leaf_value = -value  # 转换为当前节点视角        │  │
│  └──────────────────────────────────────────────────────┘  │
│                         │                                   │
│                         ▼                                   │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  4️⃣ Backpropagation (回溯)                          │  │
│  │     将价值沿路径反向传播到根节点                      │  │
│  │     node.update_recursive(-next_value)               │  │
│  │     每层自动翻转符号 (Negamax)                        │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│              根据访问次数计算动作概率                         │
│                                                             │
│  visits = [n1, n2, n3, ...]  # 每个子节点的访问次数        │
│  probs = softmax(log(visits) / temp)                       │
│                                                             │
│  训练阶段: temp=1.0, 根据概率分布采样                       │
│  实战阶段: temp≈0,  选择访问次数最多的动作                  │
└─────────────────────────────────────────────────────────────┘
```

---

## 四个核心步骤详解

### 1️⃣ Selection (选择阶段)

#### 目标
从根节点开始，沿着搜索树向下选择节点，直到到达**叶子节点**（没有子节点的节点）。

#### UCT 公式

```
UCT(s, a) = Q(s, a) + c_puct × P(s, a) × sqrt(N(s)) / (1 + N(s, a))
            ────────   ────────────────────────────────────────
            利用项              探索项

其中：
  Q(s, a)  = 动作 a 的平均价值（Exploitation）
  P(s, a)  = 神经网络给出的先验概率（Prior）
  N(s)     = 父节点的访问次数
  N(s, a)  = 子节点的访问次数
  c_puct   = 探索常数（默认 5）
```

#### 选择策略

```python
def select(self, c_puct):
    """在所有子节点中选择 UCT 值最大的"""
    return max(self._children.items(),
               key=lambda act_node: act_node[1].get_value(c_puct))
```

#### 核心逻辑

```python
node = self._root
while not node.is_leaf():
    action, node = node.select(c_puct)  # 选择 UCT 最大的子节点
    state.do_move(action)               # 在棋盘上执行动作
```

#### 关键点

- **搜索树全局共享**：所有推演在同一棵树上进行
- **棋盘状态独立**：每次推演都深拷贝棋盘
- **增量式探索**：后续推演利用前面推演的结果

---

### 2️⃣ Expansion (扩展阶段)

#### 目标
当到达叶子节点且游戏未结束时，扩展该节点，为所有合法动作创建子节点。

#### 扩展逻辑

```python
if not end:  # 游戏未结束
    # 使用神经网络获取策略和价值
    next_probs, next_value = self._policy(state)

    # 扩展节点：为每个合法动作创建子节点
    node.expand(next_probs)
```

#### expand 函数详解

```python
def expand(self, action_priors):
    """
    action_priors: [(action, prob), ...]
    例如: [(0, 0.1), (1, 0.3), (2, 0.2), ...]
    """
    for action, prob in action_priors:
        if action not in self._children:
            # 创建子节点，设置先验概率
            self._children[action] = TreeNode(self, prob, action)
```

#### 关键点

- **只扩展一次**：叶子节点第一次被访问时扩展
- **先验概率**：神经网络输出的 P(s,a) 作为子节点的先验概率
- **合法动作过滤**：`policy_value_fn` 会自动过滤非法动作

---

### 3️⃣ Evaluation (评估阶段)

#### 目标
使用神经网络评估当前局面的价值，替代传统 MCTS 的随机模拟（Rollout）。

#### 神经网络输出

```python
probs, value = policy_value_fn(state)

返回：
  probs: [(action, P(s,a)), ...]  # 策略头：动作概率分布
  value: V(s) ∈ [-1, 1]           # 价值头：局面评估
```

#### 价值含义

```
V(s) > 0  →  当前玩家优势
V(s) = 0  →  均势
V(s) < 0  →  当前玩家劣势
```

#### 视角转换（Negamax 核心）

```python
# 神经网络返回的 value 是从当前玩家（对手）视角
# 需要转换为叶子节点（我方）视角
leaf_value = -next_value
```

**为什么取负号？**
- 执行 `state.do_move(action)` 后，`current_player` 已切换为对手
- 神经网络评估的是对手视角的价值
- 叶子节点需要记录自己视角的价值
- 因此：我方价值 = -对手价值

---

### 4️⃣ Backpropagation (回溯阶段)

#### 目标
将叶子节点的评估价值沿着搜索路径反向传播到根节点，更新路径上所有节点的统计信息。

#### 回溯逻辑

```python
node.update_recursive(-leaf_value)
```

#### update_recursive 详解

```python
def update_recursive(self, leaf_value):
    """递归回溯，自动翻转符号"""
    if self._parent:
        # 先递归更新父节点（符号翻转）
        self._parent.update_recursive(-leaf_value)

    # 再更新当前节点
    self.update(leaf_value)
```

#### update 函数（更新节点统计）

```python
def update(self, leaf_value):
    """更新访问次数和平均价值"""
    self._n_visits += 1
    # 增量式计算平均值
    self._Q += (leaf_value - self._Q) / self._n_visits
```

#### Negamax 符号传播

```
示例路径: Root → A → B → C (叶子节点)
叶子节点 C 的价值 = +0.8 (C 的视角)

回溯过程:
  C.update(+0.8)              # C 自己的价值
  B.update(-0.8)              # B 视角: -C的价值
  A.update(+0.8)              # A 视角: -B的价值 = +C的价值
  Root.update(-0.8)           # Root视角: -A的价值

每层自动翻转符号，确保每个节点记录自己视角的价值
```

---

## 推演示例（完全对齐代码逻辑）

### 示例场景设定

```
3×3 井字棋空棋盘:
  0 | 1 | 2
  ---------
  3 | 4 | 5
  ---------
  6 | 7 | 8

current_player = 1 (玩家1先手，记为X)
可用位置: [0, 1, 2, 3, 4, 5, 6, 7, 8]
```

### 搜索树初始状态

```
Root节点:
  name = -1
  _n_visits = 0
  _Q = 0
  _P = 1.0
  _parent = None
  _children = {}  # 空，是叶子节点
```

---

## 第 1 次推演

### 入口
- 深拷贝当前棋盘（空棋盘）
- 调用 _playout() 函数

### 执行过程

#### 步骤 1: Selection（选择）

- 从 Root 节点开始
- 检查 Root 是否为叶子节点（判断 children 是否为空）
- **结果**：Root 的 children 是空字典 → 是叶子节点
- **跳出 while 循环**，不执行任何 select 和 do_move
- **当前状态**：node = Root，棋盘仍是空的

---

#### 步骤 2: 检查游戏结束

- 调用 game_end() 检查游戏是否结束
- **结果**：游戏未结束（空棋盘）

---

#### 步骤 3: Expansion（扩展）

- 调用神经网络评估当前局面（玩家1视角，空棋盘）
- **神经网络返回**：
  - 策略分布：9个位置的概率（中心位置4的概率稍高：0.12，其他约0.11）
  - 价值评估：0.0（空棋盘，均势）
- 执行扩展：为 Root 创建 9 个子节点，每个子节点的先验概率 P 对应神经网络输出

**扩展后的搜索树**：
```
                Root (N=0, Q=0)
                /  /  |  |  \  \  \  \  \
              0  1  2  3  4  5  6  7  8
           P=0.11  ..  .. 0.12 ..  ..  0.11
           N=0          N=0          N=0
           Q=0          Q=0          Q=0
```

---

#### 步骤 4: Backpropagation（回溯）

- 计算叶子节点价值：leaf_value = -next_value = -0.0 = 0.0
- 执行递归回溯：
  - 检查 Root 的父节点：不存在（Root._parent = None）
  - 不执行父节点更新
  - 更新 Root 节点：
    - 访问次数：0 → 1
    - 平均价值 Q：0.0（增量式计算）

**第1次推演后的搜索树**：
```
                Root (N=1, Q=0.0)
                /  /  |  |  \  \  \  \  \
              0  1  2  3  4  5  6  7  8
           P=0.11  ..  .. 0.12 ..  ..  0.11
           N=0          N=0          N=0
           Q=0          Q=0          Q=0
```

---

## 第 2 次推演

### 入口
- 重新深拷贝棋盘（仍是空棋盘）
- 调用 _playout() 函数

### 执行过程

#### 步骤 1: Selection（选择）

- 从 Root 节点开始
- 检查 Root 是否为叶子节点
- **结果**：Root 现在有 9 个子节点 → 不是叶子节点 → 进入 while 循环
- 执行 select：计算所有子节点的 UCT 值，选择最大的
  - **UCT 计算**（c_puct=5, Root.N=1）：
    - 节点 0-3, 5-8: UCT = 0 + 5 × 0.11 × √1 / 1 = 0.55
    - 节点 4: UCT = 0 + 5 × 0.12 × √1 / 1 = **0.60**（最大）
  - **选择结果**：action=4, node=节点4
- 执行棋盘动作：在位置 4 落子（玩家1在中心落子）
- 棋盘状态更新：X 在中心，current_player 切换为玩家2
- 继续检查：节点4 是叶子节点 → 退出 while 循环

**当前棋盘**：
```
  0 | 1 | 2
  ---------
  3 | X | 5    ← 玩家1在中心
  ---------
  6 | 7 | 8
```

---

#### 步骤 2: 检查游戏结束

- 检查游戏是否结束
- **结果**：只下了 1 子，游戏未结束

---

#### 步骤 3: Expansion（扩展）

- 调用神经网络评估（玩家2视角，X在中心）
- **神经网络返回**：
  - 策略分布：8 个位置（位置4已被占据），四个角位置概率稍高（0.13），其他位置约 0.11
  - 价值评估：-0.1（玩家2视角：X占中心略有优势）
- 执行扩展：为节点4创建 8 个子节点

**扩展后的搜索树**：
```
          Root (N=1, Q=0.0)
           |
           4 (N=0, Q=0, P=0.12)
         / | | | \ \ \ \
        0 1 2 3 5 6 7 8
     P=0.13 0.11 0.13 ... 0.13
     N=0                    N=0
```

---

#### 步骤 4: Backpropagation（回溯）

- 计算叶子节点价值：leaf_value = -next_value = -(-0.1) = **+0.1**
- **回溯路径**：节点4 → Root
- 执行递归回溯：
  - **先更新父节点（Root）**：
    - 传入价值：-leaf_value = -0.1
    - Root 访问次数：1 → 2
    - Root 平均价值 Q：0.0 → -0.05（增量计算）
  - **再更新当前节点（节点4）**：
    - 传入价值：+0.1
    - 节点4 访问次数：0 → 1
    - 节点4 平均价值 Q：0.0 → +0.1

**第2次推演后的搜索树**：
```
          Root (N=2, Q=-0.05)
           |
           4 (N=1, Q=+0.1, P=0.12)
         / | | | \ \ \ \
        0 1 2 3 5 6 7 8
     P=0.13 0.11 0.13 ... 0.13
     N=0                    N=0
```

---

## 第 3 次推演

### 入口
- 重新深拷贝棋盘（仍是空棋盘）
- 调用 _playout() 函数

### 执行过程

#### 步骤 1: Selection（选择）

- 从 Root 节点开始
- 检查 Root 是否为叶子节点
- **结果**：Root 有子节点 → 不是叶子节点 → 进入 while 循环
- 执行 select：计算所有子节点的 UCT 值，选择最大的
  - **UCT 计算**（c_puct=5, Root.N=2）：
    - 节点 0-3, 5-8: UCT = 0 + 5 × 0.11 × √2 / 1 ≈ **0.777**（最大）
    - 节点 4: UCT = 0.1 + 5 × 0.12 × √2 / 2 ≈ 0.524
  - **选择结果**：多个节点UCT相同，随机选一个，假设选 action=0, node=节点0
- 执行棋盘动作：在位置 0 落子（玩家1在左上角落子）
- 棋盘状态更新：X 在左上角，current_player 切换为玩家2
- 继续检查：节点0 是叶子节点 → 退出 while 循环

**当前棋盘**：
```
  X | 1 | 2    ← 玩家1在左上角
  ---------
  3 | 4 | 5
  ---------
  6 | 7 | 8
```

---

#### 步骤 2: 检查游戏结束

- 检查游戏是否结束
- **结果**：游戏未结束

---

#### 步骤 3: Expansion（扩展）

- 调用神经网络评估（玩家2视角，X在左上角）
- **神经网络返回**：
  - 策略分布：8 个位置，中心（位置4）和对角（位置8）概率较高
  - 价值评估：-0.2（玩家2视角：X先手略有优势）
- 执行扩展：为节点0创建 8 个子节点

---

#### 步骤 4: Backpropagation（回溯）

- 计算叶子节点价值：leaf_value = -next_value = -(-0.2) = **+0.2**
- **回溯路径**：节点0 → Root
- 执行递归回溯：
  - **先更新父节点（Root）**：
    - 传入价值：-leaf_value = -0.2
    - Root 访问次数：2 → 3
    - Root 平均价值 Q：-0.05 → -0.1（增量计算）
  - **再更新当前节点（节点0）**：
    - 传入价值：+0.2
    - 节点0 访问次数：0 → 1
    - 节点0 平均价值 Q：0.0 → +0.2

**第3次推演后的搜索树**：
```
          Root (N=3, Q=-0.1)
          /              |
         0               4
      N=1 Q=0.2       N=1 Q=+0.1
      P=0.11          P=0.12
      /|\...          /|\...
   子节点(8个)      子节点(8个)
```

---

## 推演1000次后

经过1000次推演，搜索树会变得非常完善：

```
                Root (N=1000, Q=-0.02)
              / / | | | \ \ \ \
            0  1  2 3 4  5 6 7 8
         N=80 70 80 70 350 70 80 70 80
         Q=0.15...  Q=0.25 ...

节点4 (N=350) - 访问次数最多
  ├── 0 (N=80, Q=0.3)
  ├── 2 (N=90, Q=0.2)
  ├── 6 (N=85, Q=0.25)
  └── 8 (N=95, Q=0.22)
  ...
```

### 最终决策

```python
# 获取所有子节点的访问次数
acts = [0, 1, 2, 3, 4, 5, 6, 7, 8]
visits = [80, 70, 80, 70, 350, 70, 80, 70, 80]

# 训练阶段 (temp=1.0)
act_probs = softmax(log(visits) / temp)
# [0.08, 0.07, 0.08, 0.07, 0.38, 0.07, 0.08, 0.07, 0.08]
action = np.random.choice(acts, p=act_probs)  # 38%概率选4

# 实战阶段 (temp≈0)
action = acts[np.argmax(visits)]  # 直接选4（中心位置）
```

---

## 关键逻辑解析

### 1. 单次推演主流程

_playout() 函数执行一次完整的推演，包含四个阶段：

**阶段 1: Selection（选择）**
- 从根节点开始，循环检查当前节点是否为叶子节点
- 如果不是叶子节点，调用 select() 计算所有子节点的 UCT 值，选择最大的
- 在棋盘上执行对应的动作，继续向下遍历
- 直到到达叶子节点为止

**阶段 2: 检查游戏是否结束**
- 调用 game_end() 判断当前局面是否已经结束（胜负或平局）

**阶段 3: Expansion + Evaluation（扩展和评估）**
- 如果游戏未结束：
  - 调用神经网络评估当前局面，获取策略分布和价值评估
  - 调用 expand() 为当前叶子节点创建所有可用动作的子节点
  - 每个子节点的先验概率 P 对应神经网络输出的策略分布
  - 用神经网络价值评估的相反数作为回溯的初始值（符号翻转）
- 如果游戏已结束：
  - 使用真实的游戏结果作为回溯值（平局=0，胜利=1，失败=-1）

**阶段 4: Backpropagation（回溯）**
- 从叶子节点开始，递归向上更新所有祖先节点
- 每向上一层，价值符号翻转一次（Negamax框架）
- 更新每个节点的访问次数 N 和平均价值 Q

### 2. UCT 计算公式

get_value() 计算节点的 UCT 值，用于选择最有前景的子节点：

**公式**：UCT = Q + U
- **U = c_puct × P × √(N_parent) / (1 + N)**

**各项含义**：
- **Q（利用项）**：节点的平均价值，倾向于选择胜率高的动作
- **U（探索项）**：探索奖励，由先验概率 P 和访问次数比决定
  - c_puct：探索常数，控制探索和利用的平衡（通常取5）
  - P：神经网络给出的先验概率，优先探索网络认为好的动作
  - √(N_parent)：父节点访问次数越多，越应该探索子节点
  - (1 + N)：子节点访问次数越多，探索奖励越小

**平衡机制**：
- 访问少的节点：Q 较小但 U 较大 → 鼓励探索
- 访问多的节点：Q 收敛到真实价值，U 减小 → 倾向于利用

### 3. 递归回溯机制

update_recursive() 实现 Negamax 框架的价值回溯：

**执行顺序**：
1. 检查是否存在父节点
2. 如果存在，先递归调用父节点的 update_recursive()，传入相反数价值
3. 递归返回后，调用当前节点的 update() 更新统计信息

**update() 更新逻辑**：
- 访问次数加1：N = N + 1
- 增量式更新平均价值：Q = Q + (new_value - Q) / N
  - 这是增量平均的数学公式，等价于 Q = (Q×(N-1) + new_value) / N
  - 避免存储所有历史值，节省内存

**Negamax 核心思想**：
- 每层玩家互换，一方的收益是另一方的损失
- 父节点的价值 = -子节点的价值
- 通过递归自动完成符号翻转，每个节点从自己视角记录价值

### 4. 搜索树复用

set_root() 在对手下完一步棋后更新根节点：

**复用策略**：
- 如果对手的动作在当前根节点的子节点中：
  - 将对应子节点提升为新的根节点
  - 断开父节点链接（置为 None）
  - 保留整个子树的搜索结果
- 如果对手的动作不在子节点中（罕见情况）：
  - 重新创建根节点，搜索树从头开始

**复用的好处**：
- 避免重复计算已经探索过的分支
- 大幅提升推演效率
- 每回合只需额外推演对手动作后的新局面
- 提高训练效率
- 适用于自我对弈（同一模型）

**何时重置搜索树**：
- 对手使用不同策略（如 vs 纯 MCTS）
- 人机对战
- 开始新局游戏

---

## 常见问题解答

### Q1: 为什么棋盘要深拷贝，搜索树不拷贝？

**A**:
- **棋盘深拷贝**：每次推演都是虚拟的，不能影响真实棋盘
- **搜索树共享**：多次推演逐步完善同一棵树，后续推演利用前面的结果

```python
# 每次推演
state_copy = copy.deepcopy(state)  # 棋盘独立
self._playout(state_copy)          # 但搜索树共享
```

---

### Q2: 叶子节点可能被访问但未扩展吗？

**A**: 不可能。叶子节点的定义是"没有子节点"。

**流程**：
1. 第 N 次推演到达节点 A（叶子节点）
2. 立即扩展节点 A，创建子节点
3. 第 N+1 次推演时，节点 A 已经不是叶子节点

---

### Q3: 为什么最终落子基于访问次数而不是 Q 值？

**A**: 访问次数融合了两个维度的信息：

1. **价值高** → MCTS 认为这个动作好 → 多次访问
2. **不确定性** → UCT 鼓励探索 → 增加访问

单纯的 Q 值可能因为样本少而不可靠，访问次数更鲁棒。

```python
# 训练阶段
probs = softmax(log(visits) / temp)  # 基于访问次数分布

# 实战阶段
action = argmax(visits)  # 选择访问最多的
```

---

### Q4: 神经网络在哪里被调用？

**A**: 在 Expansion 阶段，当到达叶子节点且游戏未结束时：

```python
if not end:
    # 调用神经网络
    probs, value = self._policy(state)

    # probs: 策略头 - 用于扩展子节点（先验概率）
    # value: 价值头 - 用于回溯（局面评估）
```

**每次推演只调用一次神经网络**（在叶子节点）。

---

### Q6: 推演次数越多越好吗？

**A**: 是的，但有边际效应递减：

```
推演次数    决策质量    计算时间
  100        ★★☆☆☆      0.1s
  400        ★★★☆☆      0.5s
  1000       ★★★★☆      1.5s
  1600       ★★★★★      2.5s
  5000       ★★★★★+     8s
```

**实践建议**：
- **训练**：400-1000 次（平衡质量和速度）
- **实战**：1000-1600 次（追求最优决策）
- **展示**：100-400 次（快速响应）

---

### Q7: 什么情况下 Q 值会是负数？

**A**: Q 值表示从当前节点视角的平均价值：

```
Q > 0  →  当前节点的玩家处于优势
Q = 0  →  均势
Q < 0  →  当前节点的玩家处于劣势
```

**例子**：
```
Root (玩家1视角): Q = -0.2  → 玩家1劣势
  └─ 节点 A (玩家2视角): Q = +0.2  → 玩家2优势
```

---

### Q8: 温度参数 temp 如何影响决策？

**A**:

```python
probs = softmax(log(visits) / temp)

temp → 0:  决策更确定（选择访问最多的）
  visits = [100, 300, 50]
  probs ≈ [0.0, 1.0, 0.0]  # 接近 one-hot

temp = 1.0: 决策更平滑（按访问次数比例）
  visits = [100, 300, 50]
  probs ≈ [0.22, 0.67, 0.11]

temp → ∞:  接近均匀分布（完全随机）
  probs ≈ [0.33, 0.33, 0.33]
```

**训练时** `temp=1.0`：增加探索性，生成多样化数据
**实战时** `temp≈0`：确定性选择，追求最优

---

## 总结

### MCTS 推演的核心要点

1. **四步循环**：Selection → Expansion → Evaluation → Backpropagation
2. **搜索树共享**：增量式探索，逐步完善
3. **UCT 平衡**：利用（Q）vs 探索（U）vs 先验（P）
4. **Negamax 框架**：符号自动翻转，简化价值传播
5. **访问次数决策**：比 Q 值更鲁棒

### 推演的意义

通过 1000 次虚拟推演，MCTS 能够：
- **预见未来**：探索多步之后的局面
- **量化价值**：通过统计找到最优动作
- **平衡探索**：不会陷入局部最优

### AlphaZero 的创新

传统 MCTS：随机模拟 → 需要大量推演
AlphaZero：神经网络评估 → 效率提升 10-100 倍

---

**最后更新时间**：2026-02-08

