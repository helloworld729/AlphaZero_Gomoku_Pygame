# v1
所以，模型到底是怎么训练的呢？
训练阶段：
「模型为 MCTS 提供先验知识（减少无效搜索），MCTS 为模型提供优化目标（提升模型性能）」，双向赋能，共同生成最优策略。

实战阶段：
「模型提供精准先验知识，MCTS 基于先验知识做深度推演，最终输出确定性最优落子」，不再追求探索性，只追求决策的准确性。


【UCT在select中使用，用于选择推演路径】---【最终实际棋盘的落子是基于访问次数】


② 理解难点
推演会对搜索树执行扩展操作，mcts全局共享，其扩展并不会因为棋盘的深拷贝解耦而解耦，即多次推演之间并非完全解耦。
例如同一落子时间步下，第N次推演执行了叶子节点A的扩展操作，第N+1看到的节点A就不是叶子节点了。
由于探索的存在，推演结束后 实际选择的节点可能是访问扩展过的，也可能仅扩展未访问。因此下一落子时间步推演的时候，
第一次推演可能面向的是叶子节点/非叶子节点。

虽然棋盘在深拷贝，但是树没有拷贝，一直在一颗上上推演。


！！！为什么价值回溯的时候，叶子节点的leaf_value取负数？
① 无论怎么路由(实际走了很多次/直接遇到叶子结点), break的时候的node肯定是叶子节点
② 叶子节点的价值没有直接算出来(这里隐含了一点:能走到叶子节点,则一定执行了do_move函数，这时候的current_player已经是node节点的对手了)
③ 在叶子节点执行策略评估，计算下一步棋(即对手棋)的 策略分布+状态价值
④ 策略分布用于执行扩展函数
⑤ node的节点价值，即叶子节点的价值，自然是状态价值(对手视角的价值)的相反数。

注：mcts的策略函数，即policy_value_fn会对非法制进行过滤。
children: act -> TreeNode
state:    act -> player



# v2
先用2e-3训练1500个epoch，在用5e-5训练1500个epoch














策略评估的时候
AlphaZero (current_mcts_player) 是玩家1
Pure MCTS (pure_mcts_player) 是玩家2
win_ratio 计算的是 win_cnt[1]，也就是玩家1（AlphaZero）的胜率
对局安排：
通过 start_player=i % 2 交替先后手
当 i 为偶数时，玩家1（AlphaZero）先手
当 i 为奇数时，玩家2（Pure MCTS）先手



create a new repository on the command line
echo "# AlphaZero_Gomoku_Pygame" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/helloworld729/AlphaZero_Gomoku_Pygame.git
git push -u origin main

or push an existing repository from the command line
git remote add origin https://github.com/helloworld729/AlphaZero_Gomoku_Pygame.git
git branch -M main
git push -u origin main


为什么策略评估的时候需要重置搜索树？
因为自我博弈的时候，两者是同一个模型，所以探索是可以用复用的
但是策略评估的时候，需要重置搜索树，因为策略评估的时候，模型是玩家2（纯MCTS），已经不是同一个师父教的了


重点看一下模型策略
特征












